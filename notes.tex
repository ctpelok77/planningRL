\documentclass[10pt]{article}

\newcommand{\commentout}[1]{}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,pdfstartview=FitH]{hyperref}


\begin{document}

\title{Notes for ICAPS 2020 Workshop Proposal}
\date{}

\author{}

\maketitle

Let's dump here some ideas or arguments that could make it or not into the proposal.
One section for person who commented.

\section*{Possible ways of do P\&L -- Scott}
In short, I see a few distinct ways to do "Planning and Learning":

\begin{itemize}
\item Model-based RL: models are always approximations of the world, but samples in RL presumably come from the real world.  How can the online experience samples improve the model and how can search in the model be used to reduce sample complexity.  Both are major topics studied under the umbrella of model-based RL.
\item Learn a model and plan in it... what I call "data-driven" planning.  This is less closed-loop than model-based RL, but still poses issues with "unusual" types of models that are difficult to plan with.
\item Use planning for some aspect of a learning process... not a huge research area, but I've seen a few interesting papers proposing something like this.
\end{itemize}

\section*{More on Methodology – Hector}

For this one, I'd like to emphasize the problems and methodology.

A big confusion, for me, it's the correlate of basic RL methods is not planning but search, as in SOCS.
By using function approximation, RL is bypassing the scalability
by assuming that similarly approximated states would have the same policy.
DeepRL, in turn, bypass the representation problem.

In contrast, the dynamic of planning problems is known.

So, we could say to an RL audience that Planning is Game-Playing –that they know– but with
\begin{itemize}
\item single agent
\item using the same method for different games.
\item must win each game, no average behaviour. 
\end{itemize}


We could say to a Planning audience that RL is like search –that they know– but with
\begin{itemize}
\item the state structure and dynamic is unknown, 
\item you get the try again from other states,
\item finally, you can fail to get a goal are truly evaluated on the average performance of a new set of chances.
\end{itemize}

By decomposing the problems and methods, I think we should be able to isolate
variations of RL that are more appealing to the planning community and viceversa.
We should be able to detect other fertil areas in need of effective methods.

For instance, consider black-box planing (no PDDL but a simulator, like required by novelty-based algorithms).
Even for that case, RL and Planning are not the same.
How can we even the evaluation so average behaviour + approximation of RL is useful?

A related question has some people in OR trying to use ML:
How to evaluate ML methods relevant to solving a single instance of an OR problem?

\section*{Beyond planning and RL – Hector}

I think  that the gap to be reduced goes all the way into factored spaces and search. For instance, the importance –and lack of– of dead-ends is also crucial in search and –perhaps surprisingly– in ML for factored space. For example, in NLP entity linking/extraction the text is to be connected with entities, but sometimes the output does not make sense. The problem extends into ML with rich factored outputs, or that secretly includes such structures –like NLP language generation. 



\end{document}
